# ğŸ¤”Alignment-for-audio-feature-and-video-frames ğŸ‘
![Language](https://img.shields.io/badge/language-c++-brightgreen) 
## ğŸ˜™1.Motivationâ“

+ åœ¨Talking Headä»»åŠ¡ä¸­ï¼Œéœ€è¦æ»¡è¶³éŸ³é¢‘è§†é¢‘åŒæ­¥
+ éŸ³é¢‘ä¸»è¦é€å…¥ç½‘ç»œçš„æ˜¯éŸ³é¢‘ç‰¹å¾ï¼Œå¸¸è§çš„éŸ³é¢‘ç‰¹å¾

  + æœªç»è¿‡ç¥ç»ç½‘ç»œå¤„ç†çš„ç‰¹å¾
    + Mel é¢‘è°±
    + MFCCç³»æ•°
  + ç»è¿‡ç¥ç»ç½‘ç»œå¤„ç†çš„ç‰¹å¾
    + HuBertç‰¹å¾
    + Wav2Vec2å¾—åˆ°çš„ç‰¹å¾
    + Deepspeechç‰¹å¾

## 2.å¯¹äºWav2Vec2ç‰¹å¾åšä¸€ä¸ªç®€å•çš„è®¨è®º

### 2.1å¯¹äºæµ‹è¯•éŸ³é¢‘

+ é¦–å…ˆè·å–éŸ³é¢‘çš„å…·ä½“æ—¶é•¿

```python
audio_file_path = "/mnt/sdb/cxh/liwen/Imitator/assets/demo/audio1.wav"  
speech_array, sampling_rate = librosa.load(audio_file_path, sr=16000)

duration = librosa.get_duration(speech_array, sampling_rate)
duration_in_ms = duration * 1000
time = str(datetime.timedelta(milliseconds=duration_in_ms))
print(time)  # 0:00:03.833375
```

### 2.2ç„¶åè§‚å¯ŸWav2Vec2çš„ç½‘ç»œç»“æ„

```python
Wav2Vec2Model(
  (feature_extractor): Wav2Vec2FeatureEncoder(
    (conv_layers): ModuleList(
      (0): Wav2Vec2GroupNormConvLayer(
        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
        (activation): GELUActivation()
        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)
      )
      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(
        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (activation): GELUActivation()
      )
      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(
        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
        (activation): GELUActivation()
      )
    )
  )
  # 5*2*2*2*2*2*2 = 320
  # 320/16000=0.02
  (feature_projection): Wav2Vec2FeatureProjection(
    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (projection): Linear(in_features=512, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Wav2Vec2Encoder(
    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(
      (conv): ParametrizedConv1d(
        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16
        (parametrizations): ModuleDict(
          (weight): ParametrizationList(
            (0): _WeightNorm()
          )
        )
      )
      (padding): Wav2Vec2SamePadLayer()
      (activation): GELUActivation()
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (layers): ModuleList(
      (0-11): 12 x Wav2Vec2EncoderLayer(
        (attention): Wav2Vec2Attention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.1, inplace=False)
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (feed_forward): Wav2Vec2FeedForward(
          (intermediate_dropout): Dropout(p=0.1, inplace=False)
          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
          (output_dense): Linear(in_features=3072, out_features=768, bias=True)
          (output_dropout): Dropout(p=0.1, inplace=False)
        )
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
```

### 2.3å¯ä»¥å¤§è‡´åˆ†ä¸ºå‡ ä¸ªæ¨¡å—ï¼š

+ é¦–å…ˆå¯¹éŸ³é¢‘ç‰¹å¾è¿›è¡Œæå–ï¼Œç”¨äº†ä¸€ä¸ª`feature_extractor`æ¨¡å—å¯¹éŸ³é¢‘ç‰¹å¾è¿›è¡Œæå–å—ï¼Œè¿™ä¸ªåœ°æ–¹çš„è¯ï¼Œç”¨åˆ°äº†7ä¸ªå·ç§¯å±‚ï¼Œå¯¹æ—¶é—´åºåˆ—çš„ç¼©å°å€æ•°æ˜¯5*2^6=320
+ åç»­çš„å¤„ç†æ¨¡å—éƒ½æ˜¯å¯¹è¿™ä¸ªæ—¶é—´æ­¥å¾—åˆ°çš„ç‰¹å¾è¿›è¡Œè¿›ä¸€æ­¥çš„åå¤„ç†æ“ä½œï¼Œä¸»è¦æ˜¯å¢åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ã€æ”¹å–„ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›ç­‰
+ å®˜æ–¹æ–‡æ¡£ä¸­å¯¹äº`Wav2Vec2Model`æ¨¡å‹è¿”å›å€¼çš„æè¿°

> A [`BaseModelOutput`](https://huggingface.co/transformers/v4.7.0/main_classes/output.html#transformers.modeling_outputs.BaseModelOutput) or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various elements depending on the configuration ([`Wav2Vec2Config`](https://huggingface.co/transformers/v4.7.0/model_doc/wav2vec2.html#transformers.Wav2Vec2Config)) and inputs.
>
> - **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) â€“ Sequence of hidden-states at the output of the last layer of the model.
>
> - **hidden_states** (`tuple(torch.FloatTensor)`, optional, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) â€“ Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
>
>   Hidden-states of the model at the output of each layer plus the initial embedding outputs.
>
> - **attentions** (`tuple(torch.FloatTensor)`, optional, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) â€“ Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

+ å¯ä»¥çœ‹åˆ°è¾“å‡º`last_hidden_state `çš„ç¬¬äºŒä¸ªç»´åº¦æ˜¯`sequence_length`
+ éªŒè¯çŒœæƒ³ï¼Œç›´æ¥åœ¨Transformeræ¨¡å—çš„`Wav2Vec2Model`çš„``feature_extractor``ä¸Šè¾“å‡ºç»´åº¦ï¼Œè§‚å¯Ÿä¸æœ€åå¾—åˆ°çš„**last_hidden_state** çš„ç»´åº¦æ˜¯å¦ç›¸ç­‰

### 2.4å¯¹ç»´åº¦è¿›è¡Œæµ‹è¯•

+ åœ¨Transformeræºç æ¨¡å—åŠ ä¸Šç»´åº¦ä¿¡æ¯æ‰“å°

```python
        hidden_states, extract_features = self.feature_projection(extract_features)
        print("hidden_states",hidden_states.shape)
        print("extract_features", extract_features.shape)
        hidden_states = self._mask_hidden_states(
```

+ å¾—åˆ°ç»´åº¦ç»“æœï¼Œè¯æ˜åé¢æ¨¡å—çš„å¤„ç†å¯¹`sequence_length`å¹¶æ²¡æœ‰å½±å“

```python
hidden_states 		torch.Size([1, 191, 768])
extract_features 	torch.Size([1, 191, 512])
last_hidden_state 	torch.Size([1, 191, 768])
```



### 2.5åºåˆ—é•¿åº¦ç»´åº¦ä¸æ—¶é—´çš„å¯¹åº”å…³ç³»

+ æ ¹æ®å‰é¢æåˆ°çš„å·ç§¯å±‚ï¼Œç‰¹å¾å·ç§¯å¯¹éŸ³é¢‘æ—¶é—´ç»´åº¦ç¼©å°å€æ•°ä¸º320

+ ä¹Ÿå°±æ˜¯è¯´å¯¹äº`16KHZ`é‡‡æ ·ç‡çš„éŸ³é¢‘ï¼Œå·ç§¯è¿‡åç¼©å°320å€ï¼Œå¾—åˆ°çš„é‡‡æ ·ç‡æ˜¯`16000/320=50`

+ ä¹Ÿå°±æ˜¯å¦‚æœè¾“å…¥`1s`çš„éŸ³é¢‘å¾—åˆ°çš„è¿™ä¸ªåºåˆ—é•¿åº¦ç»´åº¦å¯¹åº”çš„æ—¶é—´åº”è¯¥æ˜¯`1s/50=0.02s`

+ æœ€åçš„`last_hidden_state`çš„ç»´åº¦ä¹Ÿåº”è¯¥æ˜¯`0.02s`å³è§†é¢‘å¸§çš„`0.5å¸§ï¼ˆ`25fpsï¼‰

+ **ä½æ„ï¼š**

  + æœ‰ä¸€ä¸ªæ“ä½œä¼šå¯¹éŸ³é¢‘é•¿åº¦è¿›è¡Œæ”¹å˜

  + ```python
    input_values = processor(speech_array, return_tensors="pt", padding=True) # batch=1çš„è¯ï¼Œpaddingæ— æ•ˆ
    ```

  + å®˜æ–¹æ–‡æ¡£è§£é‡Š

  + >**When used Wav2Vec2Processor  in normal mode, this method forwards all its arguments to Wav2Vec2FeatureExtractorâ€™s __call__()**
    >
    >- **raw_speech** (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`) â€“ The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float values, a list of numpy arrays or a list of list of float values.
    >
    >- **padding** (`bool`, `str` or [`PaddingStrategy`](https://huggingface.co/transformers/v4.7.0/internal/file_utils.html#transformers.file_utils.PaddingStrategy), optional, defaults to `False`) â€“
    >
    >  Select a strategy to pad the returned sequences (according to the modelâ€™s padding side and padding index) among:
    >
    >  - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence if provided).
    >  - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum acceptable input length for the model if that argument is not provided.
    >  - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different lengths).
    >
    >  
    >
    >- **max_length** (`int`, optional) â€“ Maximum length of the returned list and optionally padding length (see above).
    >
    >- **pad_to_multiple_of** (`int`, optional) â€“
    >
    >  If set will pad the sequence to a multiple of the provided value.
    >
    >  This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.
    >
    >  
    >
    >- **return_attention_mask** (`bool`, optional) â€“
    >
    >  Whether to return the attention mask. If left to the default, will return the attention mask according to the specific feature_extractorâ€™s default.
    >
    >- **return_tensors** (`str` or [`TensorType`](https://huggingface.co/transformers/v4.7.0/internal/file_utils.html#transformers.file_utils.TensorType), optional) â€“
    >
    >  If set, will return tensors instead of list of python integers. Acceptable values are:
    >
    >  - `'tf'`: Return TensorFlow `tf.constant` objects.
    >  - `'pt'`: Return PyTorch `torch.Tensor` objects.
    >  - `'np'`: Return Numpy `np.ndarray` objects.
    >
    >  
    >
    >- **sampling_rate** (`int`, optional) â€“ The sampling rate at which the `raw_speech` input was sampled. It is strongly recommended to pass `sampling_rate` at the forward call to prevent silent errors.
    >
    >- **padding_value** (`float`, defaults to 0.0) â€“



### 2.6å®ä¾‹æµ‹è¯•

+ é’ˆå¯¹ä¸Šé¢çš„æ–¹æ³•éªŒè¯å®ä¾‹
  + 2.1è·å–çš„éŸ³é¢‘é•¿åº¦
    + `0:00:03.833375`
  + å¾—åˆ°çš„`embedding` `(last_hidden_state)`ç»´åº¦
    + `torch.Size([1, 191, 768])`
  + è®¡ç®—æ—¶é—´
    + `191*0.02=3.82`
    + æ—¶é—´èƒ½å¯¹ä¸Š



## 3.Melé¢‘è°±å¯¹é½çš„æ¢ç©¶

+ pass

## 4.MFCCæ¢ç©¶

+ pass

## 5.DeepSpeechæ¢ç©¶

+ pass
ğŸ“License
